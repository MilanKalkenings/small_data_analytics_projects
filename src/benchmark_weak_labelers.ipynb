{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def benchmark_labelers(\n",
    "        labelers_high_precision_pos: List[int],\n",
    "        labelers_high_precision_neg: List[int],\n",
    "        labels: List[List[int]],\n",
    "        consider_disagreeing_benchmark_labels: bool = True,\n",
    "        pos_benchmark_by_majority: bool = False,\n",
    "        neg_benchmark_by_majority: bool = False,\n",
    "        labeler_names_to_benchmark: List[str] = None, \n",
    "        labeler_names: List[str] = None):\n",
    "    \"\"\"\n",
    "    given labels and the ids of labelers with trusty positive labels and the ids of labelers with trusty negative labels, \n",
    "    this function estimates ground truth labels and provides metrics for estimating the reliability of the labelers.\n",
    "\n",
    "    background:\n",
    "    In many use cases, some labelers are known to be more trustworthy, while the performance of others is unknown.\n",
    "    This is especially the case if some labels are costly assigned by human annotators and others by cheap heuristics.\n",
    "    Due to vast amounts of data to be labeled, even manually evaluating labels provdied by all labelers can be out of scope. \n",
    "    Goal of this micro project is to provide a practical heuristic for a heuristic initial evaluation of labelers.\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    parameters:\n",
    "    labelers_high_precision_pos: List[int]\n",
    "    ids of labelers that are expected to make few mistakes when predicting positive labels\n",
    "\n",
    "    labelers_high_precision_neg: List[int]\n",
    "    ids of labelers that are expected to make few mistakes when predicting negative labels\n",
    "\n",
    "    labels: List[List[int]]\n",
    "    labels of all labelers including the high precision labelers for both classes.\n",
    "    labels[i][j] = label assigned by labeler <i> to data point <j>\n",
    "\n",
    "    consider_disagreeing_benchmark_labels: bool = True\n",
    "    for data point i, labelers_high_precision_pos predict a positive label and labelers_high_precision_neg predict a negative label.\n",
    "    if consider_disagreeing_benchmark_labels is True, both labels are considered \"correct\" in the benchmark.\n",
    "    if consider_disagreeing_benchmark_labels is False, the data point is considered to be abstain.\n",
    "\n",
    "    pos_benchmark_by_majority: bool = False\n",
    "    if True: the benchmark label is considered to be positive, if the majority vote of the labelers_high_precision_pos is positive\n",
    "    if False: the benchmark label is considered to be positive, if any of the labelers_high_precision_pos predicts positive\n",
    "\n",
    "    neg_benchmark_by_majority: bool = False\n",
    "    if True: the benchmark label is considered to be negative, if the majority vote of the labelers_high_precision_neg is negative\n",
    "    if False: the benchmark label is considered to be negative, if any of the labelers_high_precision_neg predicts negative\n",
    "\n",
    "    labeler_names: List[str] = None\n",
    "    names of the labelers which should be benchmarked against the estimated benchmark labels\n",
    "\n",
    "    labeler_names_to_benchmark: List[str] = None\n",
    "    names/string_ids of the labelers\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    Result interpretation (for positive labeling):\n",
    "    Labeler has: high Precision + high Recall + P_Additional > 0: \n",
    "    Labeler performs similar to the high precision benchmark labelers on the benchmarked observations and is thus likely trustworthy.\n",
    "    The additional positive lables are thus more likely to be trusteds.\n",
    "\n",
    "    Labeler has: high Precision + low Recall + P_Additional > 0:\n",
    "    Labeler precisely predicts positive labels on benchmarked observations and thus provides positive labels that are likely trustworthy.\n",
    "    The additional positive labels are thus more likely to be trusteds.\n",
    "    Additional negative labels should not be trusted.\n",
    "\n",
    "    Labeler has: low Precision + high Recall + P_Additional > 0:\n",
    "    Labeler often predicts positive label but produces many false positives.\n",
    "    If a data point is labeled as negative, it is unlikely that the true label is postive.\n",
    "    The positive labels from this labeler on the other hand don't provide much information.\n",
    "\n",
    "    The higher P_Abstain, the less reliable are the labelers Precision and Recall.\n",
    "    The higher N_Abstain, the less reliable is the labelers Recall.\n",
    "\n",
    "    note: \n",
    "    Precision = TP / (TP + FP)\n",
    "    Recall = TP / (TP + FN) \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    example usage:\n",
    "    labeler_names = [\n",
    "        \"Labeler with High Precision Pos 1\", \n",
    "        \"Labeler with High Precision Pos 2\",  \n",
    "        \"Labeler with High Precision Neg 1\", \n",
    "        \"Labeler with High Precision Neg 2\", \n",
    "        \"Labeler with Perfect Labels\", \n",
    "        \"Labeler with Completely Wrong Labels\", \n",
    "        \"Unknwon Labeler 1\",\n",
    "        \"Unknwon Labeler 2\",\n",
    "        \"Unknwon Labeler 3\",\n",
    "        \"Unknwon Labeler 4\",\n",
    "        \"Unknwon Labeler 5\"] \n",
    "\n",
    "    labeler_names_to_benchmark = [\n",
    "        \"Labeler with Perfect Labels\", \n",
    "        \"Labeler with Completely Wrong Labels\", \n",
    "        \"Unknwon Labeler 1\",\n",
    "        \"Unknwon Labeler 2\",\n",
    "        \"Unknwon Labeler 3\",\n",
    "        \"Unknwon Labeler 4\",\n",
    "        \"Unknwon Labeler 5\"]\n",
    "\n",
    "    # unknown ground truth labels = [1, 0, 1, 1, 0, 0, 1]\n",
    "\n",
    "    labels = [[1, 0, -1, 1, -1, 1, 0],  #  high precision pos 1\n",
    "            [-1, -1, 1, -1, -1, -1, 1],  # high precision pos 2\n",
    "            [-1, 0, -1, -1, 0, -1, -1],  # high precision neg 1\n",
    "            [-1, 0, -1, -1, -1, 0, -1],  # high precision neg 2\n",
    "            [1, 0, 1, 1, 0, 0, 1],  # perfect\n",
    "            [0, 1, 0, 0, 1, 1, 0],  # completely wrong\n",
    "\n",
    "            [1, -1, -1, 1, 0, 1, 1],  # unknown\n",
    "            [-1, -1, -1, 1, -1, -1, 1],  # unknown\n",
    "            [-1, -1, 1, 0, 0, 0, 1],  # unkown\n",
    "            [1, 0, 1, 0, 0, 0, 0],  # unknown\n",
    "            [1, -1, 1, 1, 0, 1, 0]  # unknown\n",
    "            ]  \n",
    "\n",
    "    out = benchmark_labelers(\n",
    "        labelers_high_precision_pos=[0, 1],\n",
    "        labelers_high_precision_neg=[2, 3],\n",
    "        labels=labels,\n",
    "        labeler_names=labeler_names,\n",
    "        labeler_names_to_benchmark=labeler_names_to_benchmark)\n",
    "    benchmark_results = out[\"benchmark_result\"]\n",
    "    \"\"\"\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # benchmark \n",
    "    # pos\n",
    "    if pos_benchmark_by_majority:\n",
    "        benchmark_pos = np.where((labels != -1)[labelers_high_precision_pos], labels[labelers_high_precision_pos], np.nan)\n",
    "        benchmark_pos = mode(benchmark_pos, nan_policy=\"omit\")[0]\n",
    "        benchmark_pos = [int(i) for i in np.where(benchmark_pos)[0]]\n",
    "    else:\n",
    "        benchmark_pos = [i for i, val in enumerate(np.any(labels[labelers_high_precision_pos] == 1, axis=0)) if val]\n",
    "    # neg\n",
    "    if neg_benchmark_by_majority:\n",
    "        benchmark_neg = np.where((labels != -1)[labelers_high_precision_neg], labels[labelers_high_precision_neg], np.nan)\n",
    "        benchmark_neg = mode(benchmark_neg, nan_policy=\"omit\")[0]\n",
    "        benchmark_neg = [int(i) for i in np.where(benchmark_neg)[0]]\n",
    "    else:\n",
    "        benchmark_neg = [i for i, val in enumerate(np.any(labels[labelers_high_precision_neg] == 0, axis=0)) if val]\n",
    "\n",
    "    disagreement = list(set(benchmark_pos) & set(benchmark_neg))\n",
    "    benchmark_labels = len(set(benchmark_pos) | set(benchmark_neg))\n",
    "    \n",
    "    print(\"benchmark labels:\", benchmark_labels, \"/\", len(labels[0]))\n",
    "    print(\"benchmark disagrees in\", len(disagreement), \"label(s)\")\n",
    "\n",
    "    if not consider_disagreeing_benchmark_labels:\n",
    "        benchmark_pos = [i for i in benchmark_pos if i not in disagreement]\n",
    "        benchmark_neg = [i for i in benchmark_neg if i not in disagreement]\n",
    "    df_result_rows = []\n",
    "    for i in range(len(labels)):\n",
    "        # skip labelers with high precision if wanted\n",
    "        if labeler_names_to_benchmark is not None:\n",
    "            if labeler_names[i] in labeler_names_to_benchmark:\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                tn = 0\n",
    "                fn = 0\n",
    "                abstain_pos = 0\n",
    "                abstain_neg = 0\n",
    "                additional_pos = 0\n",
    "                additional_neg = 0\n",
    "                additional_abstain = 0\n",
    "                for j in range(len(labels[i])):\n",
    "                    if (j in benchmark_pos):\n",
    "                        if labels[i][j] == 1:\n",
    "                            tp += 1\n",
    "                        elif labels[i][j] == -1:\n",
    "                            abstain_pos +=1\n",
    "                        else:\n",
    "                            fp += 1\n",
    "                    if (j in benchmark_neg):\n",
    "                        if labels[i][j] == 0:\n",
    "                            tn += 1\n",
    "                        elif labels[i][j] == -1:\n",
    "                            abstain_neg += 1\n",
    "                        else:\n",
    "                            fn += 1\n",
    "                    if (j not in benchmark_pos) and (j not in benchmark_neg):\n",
    "                        if labels[i][j] == 1:\n",
    "                            additional_pos += 1\n",
    "                        elif labels[i][j] == 0:\n",
    "                            additional_neg += 1\n",
    "                        else:\n",
    "                            additional_abstain += 1\n",
    "                # accuracy\n",
    "                if tp + fp + tn + fn == 0:\n",
    "                    acc = 0\n",
    "                else:\n",
    "                    acc = int(np.round((tp + tn) / (tp + fp + tn + fn) * 100, 0))\n",
    "                # precision\n",
    "                if tp + fp == 0:\n",
    "                    precision = 0\n",
    "                else:\n",
    "                    precision = int(np.round(tp / (tp + fp) * 100, 0))\n",
    "                # recall\n",
    "                if tp + fn == 0:\n",
    "                    recall = 0\n",
    "                else:\n",
    "                    recall = int(np.round(tp / (tp + fn) * 100, 0))\n",
    "                df_result_rows.append([acc, precision, recall, tp, fp, abstain_pos, additional_pos, tn, fn, abstain_neg, additional_neg, additional_abstain])\n",
    "\n",
    "    df_result = pd.DataFrame(data=df_result_rows, columns=[\"Accuracy (ignoring Abstain)\", \"Precision (Ignoring Abstain)\", \"Recall (Ignoring Abstain)\", \"TP\", \"FP\", \"P_Abstain\", \"P_Additional\", \"TN\", \"FN\", \"N_Abstain\", \"N_Additional\", \"Abstain_Additional\"])\n",
    "    if labeler_names is not None:\n",
    "        df_result.index = labeler_names_to_benchmark\n",
    "    return {\"benchmark_result\": df_result, \"benchmark_pos\": benchmark_pos, \"benchmark_neg\": benchmark_neg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark labels: 7 / 7\n",
      "benchmark disagrees in 1 label(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy (ignoring Abstain)</th>\n",
       "      <th>Precision (Ignoring Abstain)</th>\n",
       "      <th>Recall (Ignoring Abstain)</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>P_Abstain</th>\n",
       "      <th>P_Additional</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>N_Abstain</th>\n",
       "      <th>N_Additional</th>\n",
       "      <th>Abstain_Additional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Labeler with Perfect Labels</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labeler with Completely Wrong Labels</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknwon Labeler 1</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknwon Labeler 2</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknwon Labeler 3</th>\n",
       "      <td>75</td>\n",
       "      <td>67</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknwon Labeler 4</th>\n",
       "      <td>67</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknwon Labeler 5</th>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Accuracy (ignoring Abstain)  \\\n",
       "Labeler with Perfect Labels                                   100   \n",
       "Labeler with Completely Wrong Labels                            0   \n",
       "Unknwon Labeler 1                                             100   \n",
       "Unknwon Labeler 2                                             100   \n",
       "Unknwon Labeler 3                                              75   \n",
       "Unknwon Labeler 4                                              67   \n",
       "Unknwon Labeler 5                                              80   \n",
       "\n",
       "                                      Precision (Ignoring Abstain)  \\\n",
       "Labeler with Perfect Labels                                    100   \n",
       "Labeler with Completely Wrong Labels                             0   \n",
       "Unknwon Labeler 1                                              100   \n",
       "Unknwon Labeler 2                                              100   \n",
       "Unknwon Labeler 3                                               67   \n",
       "Unknwon Labeler 4                                               50   \n",
       "Unknwon Labeler 5                                               75   \n",
       "\n",
       "                                      Recall (Ignoring Abstain)  TP  FP  \\\n",
       "Labeler with Perfect Labels                                 100   4   0   \n",
       "Labeler with Completely Wrong Labels                          0   0   4   \n",
       "Unknwon Labeler 1                                           100   3   0   \n",
       "Unknwon Labeler 2                                           100   2   0   \n",
       "Unknwon Labeler 3                                           100   2   1   \n",
       "Unknwon Labeler 4                                           100   2   2   \n",
       "Unknwon Labeler 5                                           100   3   1   \n",
       "\n",
       "                                      P_Abstain  P_Additional  TN  FN  \\\n",
       "Labeler with Perfect Labels                   0             0   2   0   \n",
       "Labeler with Completely Wrong Labels          0             1   0   2   \n",
       "Unknwon Labeler 1                             1             1   1   0   \n",
       "Unknwon Labeler 2                             2             0   0   0   \n",
       "Unknwon Labeler 3                             1             0   1   0   \n",
       "Unknwon Labeler 4                             0             0   2   0   \n",
       "Unknwon Labeler 5                             0             1   1   0   \n",
       "\n",
       "                                      N_Abstain  N_Additional  \\\n",
       "Labeler with Perfect Labels                   0             1   \n",
       "Labeler with Completely Wrong Labels          0             0   \n",
       "Unknwon Labeler 1                             1             0   \n",
       "Unknwon Labeler 2                             2             0   \n",
       "Unknwon Labeler 3                             1             1   \n",
       "Unknwon Labeler 4                             0             1   \n",
       "Unknwon Labeler 5                             1             0   \n",
       "\n",
       "                                      Abstain_Additional  \n",
       "Labeler with Perfect Labels                            0  \n",
       "Labeler with Completely Wrong Labels                   0  \n",
       "Unknwon Labeler 1                                      0  \n",
       "Unknwon Labeler 2                                      1  \n",
       "Unknwon Labeler 3                                      0  \n",
       "Unknwon Labeler 4                                      0  \n",
       "Unknwon Labeler 5                                      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeler_names = [\n",
    "    \"Labeler with High Precision Pos 1\", \n",
    "    \"Labeler with High Precision Pos 2\",  \n",
    "    \"Labeler with High Precision Neg 1\", \n",
    "    \"Labeler with High Precision Neg 2\", \n",
    "    \"Labeler with Perfect Labels\", \n",
    "    \"Labeler with Completely Wrong Labels\", \n",
    "    \"Unknwon Labeler 1\",\n",
    "    \"Unknwon Labeler 2\",\n",
    "    \"Unknwon Labeler 3\",\n",
    "    \"Unknwon Labeler 4\",\n",
    "    \"Unknwon Labeler 5\"] \n",
    "\n",
    "labeler_names_to_benchmark = [\n",
    "    \"Labeler with Perfect Labels\", \n",
    "    \"Labeler with Completely Wrong Labels\", \n",
    "    \"Unknwon Labeler 1\",\n",
    "    \"Unknwon Labeler 2\",\n",
    "    \"Unknwon Labeler 3\",\n",
    "    \"Unknwon Labeler 4\",\n",
    "    \"Unknwon Labeler 5\"]\n",
    "\n",
    "# unknown ground truth labels = [1, 0, 1, 1, 0, 0, 1]\n",
    "\n",
    "labels = [[1, 0, -1, 1, -1, 1, 0],  #  high precision pos 1\n",
    "        [-1, -1, 1, -1, -1, -1, 1],  # high precision pos 2\n",
    "        [-1, 0, -1, -1, 0, -1, -1],  # high precision neg 1\n",
    "        [-1, 0, -1, -1, -1, 0, -1],  # high precision neg 2\n",
    "        [1, 0, 1, 1, 0, 0, 1],  # perfect\n",
    "        [0, 1, 0, 0, 1, 1, 0],  # completely wrong\n",
    "\n",
    "        [1, -1, -1, 1, 0, 1, 1],  # unknown\n",
    "        [-1, -1, -1, 1, -1, -1, 1],  # unknown\n",
    "        [-1, -1, 1, 0, 0, 0, 1],  # unkown\n",
    "        [1, 0, 1, 0, 0, 0, 0],  # unknown\n",
    "        [1, -1, 1, 1, 0, 1, 0]  # unknown\n",
    "        ]  \n",
    "\n",
    "out = benchmark_labelers(\n",
    "    labelers_high_precision_pos=[0, 1],\n",
    "    labelers_high_precision_neg=[2, 3],\n",
    "    labels=labels,\n",
    "    labeler_names=labeler_names,\n",
    "    labeler_names_to_benchmark=labeler_names_to_benchmark,\n",
    "    consider_disagreeing_benchmark_labels=False)\n",
    "benchmark_results = out[\"benchmark_result\"]\n",
    "benchmark_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
